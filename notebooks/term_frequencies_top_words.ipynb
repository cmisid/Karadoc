{
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "metadata": {},
      "source": [
        "<!-- Table of Contents generated by Jupytoc -->\n",
        "**Table of Contents**\n",
        "- [Classifying by using top words intersections](#Classifying-by-using-top-words-intersections)\n"
      ],
      "cell_type": "markdown"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying by using top words intersections"
      ],
      "metadata": {}
    },
    {
      "execution_count": 52,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.concat(\n",
        "    (\n",
        "        pd.read_csv('../features/metadata/tf_descriptions.csv', index_col=0),\n",
        "        pd.read_csv('../features/metadata/tf_keywords.csv', index_col=0),\n",
        "        #pd.read_csv('../features/metadata/tf_titles.csv', index_col=0),\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "df = df.groupby(df.columns, axis=1).sum() # Drop duplicate columns\n",
        "df['tag'] = pd.read_csv('../features/tags.csv', index_col=0)['tag_name']"
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split train-test"
      ],
      "metadata": {}
    },
    {
      "execution_count": 92,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_indices = np.random.rand(len(df)) < 0.8\n",
        "\n",
        "train = df[train_indices]\n",
        "test = df[~train_indices]"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "execution_count": 95,
      "outputs": [],
      "cell_type": "code",
      "source": [
        "top_words_per_tag = {\n",
        "    tag: set(group.drop('tag', axis=1).sum(axis=0).nlargest(35).index)\n",
        "    for tag, group in train.groupby('tag') # Notice this is done with the training set\n",
        "}"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "execution_count": 96,
      "outputs": [
        {
          "text": [
            "0.711864406779661\n"
          ],
          "output_type": "stream",
          "name": "stdout"
        }
      ],
      "cell_type": "code",
      "source": [
        "predicted = [\n",
        "    sorted(\n",
        "        top_words_per_tag.items(),\n",
        "        key=lambda x: len(set.intersection(set(row[row > 0].index), x[1]))\n",
        "    )[-1][0]\n",
        "    for _, row in test.drop('tag', axis=1).iterrows()\n",
        "]\n",
        "\n",
        "precision = sum([\n",
        "        prediction == truth\n",
        "        for prediction, truth in zip(predicted, test['tag'].tolist())\n",
        "]) / len(predicted)\n",
        "\n",
        "print(precision)"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "version": "3.5.2",
      "nbconvert_exporter": "python",
      "file_extension": ".py",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    }
  }
}